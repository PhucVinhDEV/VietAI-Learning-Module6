# Gi·∫£i Th√≠ch: `import torch.nn as nn`

## üîç `torch.nn` l√† g√¨?

`torch.nn` l√† **Neural Network module** c·ªßa PyTorch - m·ªôt th∆∞ vi·ªán ch·ª©a c√°c building blocks (kh·ªëi x√¢y d·ª±ng) ƒë·ªÉ t·∫°o neural networks.

### T·∫°i sao d√πng `as nn`?

```python
import torch.nn as nn  # Thay v√¨ import torch.nn
```

**L√Ω do:**
- **Ng·∫Øn g·ªçn**: `nn.Linear` thay v√¨ `torch.nn.Linear`
- **Convention**: C·ªông ƒë·ªìng PyTorch d√πng `nn` nh∆∞ m·ªôt chu·∫©n
- **D·ªÖ ƒë·ªçc**: Code ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu h∆°n

---

## üì¶ C√°c Th√†nh Ph·∫ßn Quan Tr·ªçng Trong `torch.nn`

### 1. `nn.Module` - Base Class cho T·∫•t C·∫£ Models

**L√† g√¨?**
- Base class m√† t·∫•t c·∫£ neural network layers/models ph·∫£i k·∫ø th·ª´a
- Cung c·∫•p c√°c t√≠nh nƒÉng: parameter management, device placement, training/eval mode

**V√≠ d·ª• trong project:**

```python
import torch.nn as nn

class BaseForecastModel(nn.Module, ABC):  # ‚Üê K·∫ø th·ª´a t·ª´ nn.Module
    def __init__(self, seq_len: int, pred_len: int):
        super().__init__()  # ‚Üê G·ªçi constructor c·ªßa nn.Module
        self.seq_len = seq_len
        self.pred_len = pred_len
```

**T·∫°i sao c·∫ßn `nn.Module`?**
- ‚úÖ T·ª± ƒë·ªông qu·∫£n l√Ω parameters (weights, biases)
- ‚úÖ H·ªó tr·ª£ `.to(device)` ƒë·ªÉ chuy·ªÉn model l√™n GPU
- ‚úÖ H·ªó tr·ª£ `.train()` v√† `.eval()` modes
- ‚úÖ H·ªó tr·ª£ save/load models

**V√≠ d·ª• s·ª≠ d·ª•ng:**

```python
model = Linear(seq_len=30, pred_len=7)

# Xem t·∫•t c·∫£ parameters
for param in model.parameters():
    print(param.shape)

# Chuy·ªÉn l√™n GPU
model = model.to('cuda')

# Chuy·ªÉn sang eval mode (t·∫Øt dropout, batch norm updates)
model.eval()
```

---

### 2. `nn.Linear` - Linear Layer

**L√† g√¨?**
- M·ªôt fully-connected layer (dense layer) th·ª±c hi·ªán ph√©p to√°n: `y = Wx + b`
- ƒê√¢y l√† layer c∆° b·∫£n nh·∫•t trong neural networks

**C√¥ng th·ª©c:**
```
output = input √ó weight^T + bias
```

**V√≠ d·ª• trong project:**

```python
import torch.nn as nn

class Linear(BaseForecastModel):
    def __init__(self, seq_len: int, pred_len: int):
        super().__init__(seq_len, pred_len)
        
        # T·∫°o m·ªôt linear layer
        # Input size: seq_len (v√≠ d·ª•: 30)
        # Output size: pred_len (v√≠ d·ª•: 7)
        self.linear = nn.Linear(seq_len, pred_len)
        # ‚Üë
        # T∆∞∆°ng ƒë∆∞∆°ng v·ªõi:
        # - Weight matrix W: shape (pred_len, seq_len) = (7, 30)
        # - Bias vector b: shape (pred_len,) = (7,)
```

**Gi·∫£i th√≠ch chi ti·∫øt:**

```python
# Khi b·∫°n vi·∫øt:
self.linear = nn.Linear(30, 7)

# PyTorch t·ª± ƒë·ªông t·∫°o:
# - W (weight): tensor shape (7, 30) - kh·ªüi t·∫°o ng·∫´u nhi√™n
# - b (bias): tensor shape (7,) - kh·ªüi t·∫°o ng·∫´u nhi√™n

# Khi forward:
x = torch.randn(8, 30)  # (batch_size=8, input_size=30)
y = self.linear(x)      # (batch_size=8, output_size=7)

# Th·ª±c ch·∫•t l√†:
# y = x @ W.T + b
#   = (8, 30) @ (30, 7) + (7,)
#   = (8, 7)
```

**V√≠ d·ª• c·ª• th·ªÉ:**

```python
import torch
import torch.nn as nn

# T·∫°o linear layer
linear = nn.Linear(in_features=30, out_features=7)

# Input: 8 samples, m·ªói sample c√≥ 30 features
x = torch.randn(8, 30)
print(f"Input shape: {x.shape}")  # (8, 30)

# Forward pass
y = linear(x)
print(f"Output shape: {y.shape}")  # (8, 7)

# Xem weights v√† bias
print(f"Weight shape: {linear.weight.shape}")  # (7, 30)
print(f"Bias shape: {linear.bias.shape}")      # (7,)
```

---

### 3. C√°c Layer Kh√°c Trong `torch.nn` (Tham Kh·∫£o)

M·∫∑c d√π project n√†y ch·ªâ d√πng `nn.Linear`, nh∆∞ng `torch.nn` c√≤n c√≥ nhi·ªÅu layer kh√°c:

```python
import torch.nn as nn

# Convolutional layers
nn.Conv1d()  # 1D convolution (cho time series)
nn.Conv2d()  # 2D convolution (cho images)

# Activation functions
nn.ReLU()    # ReLU activation
nn.Sigmoid() # Sigmoid activation
nn.Tanh()    # Tanh activation

# Normalization
nn.BatchNorm1d()  # Batch normalization
nn.LayerNorm()    # Layer normalization

# Dropout (regularization)
nn.Dropout()      # Dropout layer

# Recurrent layers
nn.LSTM()         # LSTM layer
nn.GRU()          # GRU layer
nn.RNN()          # RNN layer

# Loss functions
nn.MSELoss()      # Mean Squared Error
nn.CrossEntropyLoss()  # Cross Entropy Loss
```

---

## üìù V√≠ D·ª• ƒê·∫ßy ƒê·ªß: T·ª´ Import ƒê·∫øn S·ª≠ D·ª•ng

### V√≠ d·ª• 1: Linear Model

```python
import torch
import torch.nn as nn  # ‚Üê Import module

class Linear(nn.Module):  # ‚Üê K·∫ø th·ª´a t·ª´ nn.Module
    def __init__(self, seq_len: int, pred_len: int):
        super().__init__()
        
        # T·∫°o linear layer
        self.linear = nn.Linear(seq_len, pred_len)
        # ‚Üë
        # nn.Linear l√† class trong torch.nn module
        # T·∫°o m·ªôt layer v·ªõi:
        # - Input: seq_len features
        # - Output: pred_len features
    
    def forward(self, x):
        return self.linear(x)  # ‚Üê G·ªçi forward c·ªßa linear layer

# S·ª≠ d·ª•ng
model = Linear(seq_len=30, pred_len=7)
x = torch.randn(8, 30)  # 8 samples, 30 features
y = model(x)            # 8 samples, 7 predictions
```

### V√≠ d·ª• 2: DLinear Model (D√πng 2 Linear Layers)

```python
import torch
import torch.nn as nn

class DLinear(nn.Module):
    def __init__(self, seq_len: int, pred_len: int):
        super().__init__()
        
        # 2 linear layers ri√™ng bi·ªát
        self.linear_trend = nn.Linear(seq_len, pred_len)      # ‚Üê Layer 1
        self.linear_seasonal = nn.Linear(seq_len, pred_len)  # ‚Üê Layer 2
    
    def forward(self, x_trend, x_seasonal):
        y_trend = self.linear_trend(x_trend)
        y_seasonal = self.linear_seasonal(x_seasonal)
        return y_trend + y_seasonal
```

---

## üéØ T√≥m T·∫Øt

### `import torch.nn as nn` l√† g√¨?

1. **`torch.nn`**: Module ch·ª©a c√°c building blocks cho neural networks
2. **`as nn`**: Alias (b√≠ danh) ƒë·ªÉ code ng·∫Øn g·ªçn h∆°n
3. **M·ª•c ƒë√≠ch**: Cung c·∫•p c√°c class nh∆∞ `nn.Module`, `nn.Linear`, etc.

### C√°c Class Quan Tr·ªçng Trong Project

| Class | M·ª•c ƒê√≠ch | V√≠ D·ª• S·ª≠ D·ª•ng |
|-------|----------|---------------|
| `nn.Module` | Base class cho t·∫•t c·∫£ models | `class MyModel(nn.Module):` |
| `nn.Linear` | Linear transformation layer | `nn.Linear(30, 7)` |

### So S√°nh: C√≥ v√† Kh√¥ng C√≥ `as nn`

```python
# Kh√¥ng d√πng as nn (d√†i d√≤ng)
import torch.nn
class Model(torch.nn.Module):
    def __init__(self):
        self.layer = torch.nn.Linear(30, 7)

# D√πng as nn (ng·∫Øn g·ªçn - RECOMMENDED)
import torch.nn as nn
class Model(nn.Module):
    def __init__(self):
        self.layer = nn.Linear(30, 7)
```

---

## üí° L∆∞u √ù Quan Tr·ªçng

1. **Lu√¥n k·∫ø th·ª´a `nn.Module`**: T·∫•t c·∫£ models ph·∫£i k·∫ø th·ª´a t·ª´ `nn.Module`
2. **G·ªçi `super().__init__()`**: Lu√¥n g·ªçi trong `__init__` c·ªßa model
3. **ƒê·ªãnh nghƒ©a `forward()`**: Method n√†y ƒë∆∞·ª£c g·ªçi khi b·∫°n g·ªçi `model(x)`
4. **Parameters t·ª± ƒë·ªông**: `nn.Module` t·ª± ƒë·ªông qu·∫£n l√Ω t·∫•t c·∫£ parameters

---

## üîó Li√™n K·∫øt

- PyTorch Documentation: https://pytorch.org/docs/stable/nn.html
- Code trong project: `src/model/linear.py`, `src/model/n_linear.py`, `src/model/d_linear.py`

